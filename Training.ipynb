{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b554af7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from src.Trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4652e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train() -> None:\n",
    "    \"\"\"Train the DQN agent on the Blackjack environment.\"\"\"\n",
    "    try:\n",
    "        seed = 42\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        trainer = Trainer(\n",
    "            n_episodes=50000,  # Total number of episodes\n",
    "            max_t=100,  # Max steps per episode\n",
    "            eps_start=1.0,  # Initial epsilon (exploration)\n",
    "            eps_end=0.05,  # Final epsilon (exploitation)\n",
    "            eps_decay=0.995,  # Epsilon decay factor (faster decay)\n",
    "            model_save_path=\"models/\",  # Where to save trained models\n",
    "            device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "            seed=seed,\n",
    "        )\n",
    "\n",
    "        # Start training\n",
    "        print(\"Starting training...\")\n",
    "        trainer.train()\n",
    "\n",
    "        # After training, save final model\n",
    "        trainer.save_model(\"dqn_blackjack_final.pth\")\n",
    "        print(\"Training completed. Final model saved.\")\n",
    "\n",
    "        # Plot scores\n",
    "        trainer.plot_scores()\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error during training: {str(e)}\") from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "139886b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Episode 100/50000, Moving Average Score: -0.29\n",
      "Episode 200/50000, Moving Average Score: -0.30\n",
      "Episode 300/50000, Moving Average Score: 0.02\n",
      "Episode 400/50000, Moving Average Score: 0.15\n",
      "Episode 500/50000, Moving Average Score: 0.33\n",
      "Episode 600/50000, Moving Average Score: 0.12\n",
      "Episode 700/50000, Moving Average Score: 0.16\n",
      "Episode 800/50000, Moving Average Score: -0.02\n",
      "Episode 900/50000, Moving Average Score: 0.08\n",
      "Episode 1000/50000, Moving Average Score: 0.25\n",
      "Episode 1100/50000, Moving Average Score: 0.21\n",
      "Episode 1200/50000, Moving Average Score: 0.04\n",
      "Episode 1300/50000, Moving Average Score: 0.17\n",
      "Episode 1400/50000, Moving Average Score: 0.21\n",
      "Episode 1500/50000, Moving Average Score: 0.22\n",
      "Episode 1600/50000, Moving Average Score: 0.25\n",
      "Episode 1700/50000, Moving Average Score: 0.23\n",
      "Episode 1800/50000, Moving Average Score: 0.10\n",
      "Episode 1900/50000, Moving Average Score: 0.34\n",
      "Episode 2000/50000, Moving Average Score: 0.34\n",
      "Episode 2100/50000, Moving Average Score: 0.12\n",
      "Episode 2200/50000, Moving Average Score: 0.31\n",
      "Episode 2300/50000, Moving Average Score: 0.24\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# After training, save final model\u001b[39;00m\n\u001b[32m     22\u001b[39m trainer.save_model(\u001b[33m\"\u001b[39m\u001b[33mdqn_blackjack_final.pth\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\juanj\\OneDrive - UPB\\NoEstructurados\\BlackjackDQL\\src\\Trainer.py:63\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     61\u001b[39m score = \u001b[32m0\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.max_t):\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m     action = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m     next_state, reward, done, _ = \u001b[38;5;28mself\u001b[39m.env.step(action)\n\u001b[32m     65\u001b[39m     next_state = np.array(next_state, dtype=np.float32)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\juanj\\OneDrive - UPB\\NoEstructurados\\BlackjackDQL\\src\\Agent.py:83\u001b[39m, in \u001b[36mAgent.act\u001b[39m\u001b[34m(self, state, eps)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Returns actions for given state as per current policy.\"\"\"\u001b[39;00m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m     state = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.unsqueeze(\u001b[32m0\u001b[39m).to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m     84\u001b[39m     \u001b[38;5;28mself\u001b[39m.qnetwork_local.eval()\n\u001b[32m     85\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
